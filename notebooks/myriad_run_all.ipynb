{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 97 118\n",
      "..\\data\\Cloud\\img_msec_1608603767961_2_half_1.png\n"
     ]
    }
   ],
   "source": [
    "# Get all data paths\n",
    "import os\n",
    "paths = {0: [], 1: [], 2: []}\n",
    "folder_names = [\"Cloud\", \"Edge\", \"Good\"]\n",
    "for i, folder_name in enumerate(folder_names):\n",
    "    folder_path = os.path.join(\"..\\data\", folder_name)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        paths[i].append(os.path.join(folder_path, file_name))\n",
    "print(len(paths[0]), len(paths[1]), len(paths[2]))\n",
    "print(paths[0][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.preprocess import PrePostProcessor, ResizeAlgorithm, ColorFormat\n",
    "from openvino.runtime import Core, Layout, Type\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model_path = 'model_mobilenet_v2.onnx'\n",
    "device_name = 'MYRIAD'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating OpenVINO Runtime Core\n"
     ]
    }
   ],
   "source": [
    "# Initialize openvino runtime core\n",
    "print('Creating OpenVINO Runtime Core')\n",
    "core = Core()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the model: model_mobilenet_v2.onnx\n"
     ]
    }
   ],
   "source": [
    "# Read ONNX model\n",
    "print(f'Reading the model: {model_path}')\n",
    "# (.xml and .bin files) or (.onnx file)\n",
    "model = core.read_model(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images\n",
      "Putting images into numpy array\n"
     ]
    }
   ],
   "source": [
    "# Read input image\n",
    "image_paths = paths[0] + paths[1] + paths[2]\n",
    "# images = [cv2.imread(path) for path in image_paths]\n",
    "# count = 0\n",
    "# for i, image in enumerate(images):\n",
    "#     if image.shape != (1942, 1024, 3):\n",
    "#         print(f\"Image {i} has shape {image.shape}\")\n",
    "#         count += 1\n",
    "# print(f\"Total {count} images have wrong shape\")\n",
    "images = []\n",
    "print(\"Reading images\")\n",
    "for path in image_paths:\n",
    "    image = cv2.imread(path)\n",
    "    # make grayscale\n",
    "    image = cv2.cvtColor(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2BGR)\n",
    "    if image.shape == (1942, 1024, 3):\n",
    "        images.append(image)\n",
    "    else:\n",
    "        images.append(cv2.resize(image, (1024, 1942)))\n",
    "print(\"Putting images into numpy array\")\n",
    "complete_tensor = np.array(images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 1\n",
    "example_input = complete_tensor[:batch_size]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Preprocess input image\n",
    "ppp = PrePostProcessor(model)\n",
    "\n",
    "_, h, w, _ = example_input.shape\n",
    "\n",
    "# 1) Set input tensor information:\n",
    "# - input() provides information about a single model input\n",
    "# - reuse precision and shape from already available `input_tensor`\n",
    "# - layout of data is 'NHWC'\n",
    "ppp.input().tensor() \\\n",
    "    .set_shape(example_input.shape) \\\n",
    "    .set_element_type(Type.u8) \\\n",
    "    .set_color_format(ColorFormat.BGR) \\\n",
    "    .set_layout(Layout('NHWC'))  # noqa: ECE001, N400\n",
    "\n",
    "dataset_mean = [0.2391, 0.4028, 0.4096]\n",
    "dataset_std = [0.2312, 0.3223, 0.3203]\n",
    "# 2) Adding explicit preprocessing steps:\n",
    "# - apply linear resize from tensor spatial dims to model spatial dims\n",
    "ppp.input().preprocess()\\\n",
    "    .resize(ResizeAlgorithm.RESIZE_LINEAR)\\\n",
    "    .convert_element_type(Type.f32)\\\n",
    "    .convert_color(ColorFormat.RGB)\\\n",
    "    .scale(255)\\\n",
    "    .mean(dataset_mean)\\\n",
    "    .scale(dataset_std)\n",
    "\n",
    "# transform = transforms.Compose([v2.ToImage(),\n",
    "#                                 v2.Resize((int(256), int(256))),\n",
    "#                                 v2.RandomHorizontalFlip(p=0.5),\n",
    "#                                 v2.RandomVerticalFlip(p=0.5),\n",
    "#                                 v2.ToDtype(torch.float32, scale=True),\n",
    "#                                 v2.Grayscale(num_output_channels=3),\n",
    "#                                 v2.Normalize(dataset_mean,dataset_std)\n",
    "#                                 ])\n",
    "# 3) Here we suppose model has 'NCHW' layout for input\n",
    "ppp.input().model().set_layout(Layout('NCHW'))\n",
    "\n",
    "# 4) Set output tensor information:\n",
    "# - precision of tensor is supposed to be 'f32'\n",
    "ppp.output().tensor().set_element_type(Type.f32)\n",
    "\n",
    "# 5) Apply preprocessing modifying the original 'model'\n",
    "processed_model = ppp.build()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model to the MYRIAD device\n",
      "Total time: 6.596s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Load the model into the device\n",
    "print(f'Loading the model to the {device_name} device')\n",
    "start = time.time()\n",
    "compiled_model = core.compile_model(processed_model, device_name)\n",
    "end = time.time()\n",
    "print(f\"Total time: {end - start:.3f}s\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference in synchronous mode\n",
      "305 / 3055\n",
      "Total time: 28.840s\n",
      "Average time: 0.095s\n"
     ]
    }
   ],
   "source": [
    "# Create infer request\n",
    "print('Starting inference in synchronous mode')\n",
    "start = time.time()\n",
    "# for i in range(iterations):\n",
    "#     results = compiled_model.infer_new_request({0: example_input})\n",
    "results = []\n",
    "for i, x in enumerate(complete_tensor):\n",
    "    y = compiled_model.infer_new_request({0: np.expand_dims(x, 0)})\n",
    "    predictions = next(iter(y.values()))\n",
    "    probs = predictions.reshape(-1)\n",
    "    results.append(probs)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"\\r {i} / {len(complete_tensor)}\", end=\"\")\n",
    "end = time.time()\n",
    "print(f\"\\r{len(complete_tensor)} / {len(complete_tensor)}\")\n",
    "total = end - start\n",
    "average = total / len(complete_tensor)\n",
    "print(f\"Total time: {total:.3f}s\")\n",
    "print(f\"Average time: {average:.3f}s\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.843\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy\n",
    "results = np.array(results)\n",
    "actual = np.array([0] * len(paths[0]) + [1] * len(paths[1]) + [2] * len(paths[2]))\n",
    "accuracy = np.mean(np.argmax(results, axis=1) == actual)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Comparison to torch model\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_path = 'model_mobilenet_v2.pth'\n",
    "model = models.mobilenet_v2()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "def test(model, test_loader, loss_fn, num_runs=5, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total_accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "        count = 0\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(inputs)\n",
    "                loss = loss_fn(output, targets)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "            count += 1\n",
    "            if count % 10 == 0:\n",
    "                print(f\"\\r{count} / {len(test_loader)}: {targets} {torch.max(F.softmax(output, dim=1), dim=1)[1]} {correct}\", end=\"\")\n",
    "        print(f\"\\r{len(test_loader)} / {len(test_loader)}\")\n",
    "        accuracy = num_correct / num_examples\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "    average_loss = total_loss / (num_runs * len(test_loader.dataset))\n",
    "    average_accuracy = total_accuracy / num_runs\n",
    "\n",
    "    print('Average Test Loss: {:.2f}, Average Test Accuracy: {:.2f}'.format(average_loss, average_accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddd\\anaconda3\\envs\\OMICRON2\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 / 77: tensor([0, 0, 0, 2]) tensor([0, 0, 0, 2]) tensor([True, True, True, True])ue])\n",
      "Average Test Loss: 0.33, Average Test Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "path_data=os.path.join(\"..\", \"data\")\n",
    "dataset_mean = [0.2391, 0.4028, 0.4096]\n",
    "dataset_std = [0.2312, 0.3223, 0.3203]\n",
    "\n",
    "transform = transforms.Compose([v2.ToImage(),\n",
    "                                v2.Resize((int(256), int(256))),\n",
    "                                v2.RandomHorizontalFlip(p=0.5),\n",
    "                                v2.RandomVerticalFlip(p=0.5),\n",
    "                                v2.ToDtype(torch.float32, scale=True),\n",
    "                                v2.Grayscale(num_output_channels=3),\n",
    "                                v2.Normalize(dataset_mean,dataset_std)\n",
    "                                ])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=path_data,\n",
    "                                 transform=transform)\n",
    "test_loader = DataLoader(dataset, batch_size=4, pin_memory=False, shuffle=True)\n",
    "test(model, test_loader, torch.nn.CrossEntropyLoss(), num_runs=1, device=\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.9934, -0.9934, -0.9934,  ..., -0.8868, -0.9018, -0.9256],\n         [-0.9118, -0.9118, -0.8967,  ..., -0.9168, -0.9098, -0.9256],\n         [-0.9118, -0.9118, -0.9118,  ..., -0.9118, -0.9018, -0.9256],\n         ...,\n         [ 3.1964,  2.7788,  2.6269,  ...,  2.8648,  3.0676,  2.7299],\n         [ 3.1578,  3.1641,  2.9206,  ...,  2.8293,  2.7735,  2.5607],\n         [-1.0342, -1.0342, -1.0342,  ...,  3.0777,  3.0169,  2.5697]],\n\n        [[-1.2205, -1.2205, -1.2205,  ..., -1.1440, -1.1548, -1.1719],\n         [-1.1620, -1.1620, -1.1512,  ..., -1.1656, -1.1606, -1.1719],\n         [-1.1620, -1.1620, -1.1620,  ..., -1.1620, -1.1548, -1.1719],\n         ...,\n         [ 1.7850,  1.4854,  1.3765,  ...,  1.5471,  1.6926,  1.4503],\n         [ 1.7573,  1.7618,  1.5871,  ...,  1.5217,  1.4817,  1.3290],\n         [-1.2498, -1.2498, -1.2498,  ...,  1.6999,  1.6562,  1.3354]],\n\n        [[-1.2494, -1.2494, -1.2494,  ..., -1.1724, -1.1833, -1.2004],\n         [-1.1904, -1.1904, -1.1796,  ..., -1.1941, -1.1891, -1.2004],\n         [-1.1904, -1.1904, -1.1904,  ..., -1.1904, -1.1833, -1.2004],\n         ...,\n         [ 1.7750,  1.4735,  1.3638,  ...,  1.5356,  1.6819,  1.4382],\n         [ 1.7471,  1.7516,  1.5758,  ...,  1.5099,  1.4697,  1.3161],\n         [-1.2788, -1.2788, -1.2788,  ...,  1.6893,  1.6453,  1.3225]]])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_loader))[0][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

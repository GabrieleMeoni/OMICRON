{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beefc598-b578-4420-ae2a-14ba2ea15a9e",
   "metadata": {},
   "source": [
    "# Train example notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe940bf-d307-4329-a62d-5b3810a293b9",
   "metadata": {},
   "source": [
    "This notebook is used to implement the training of a neural network for classification of `Cloud`, `Edge`, `Good` images. <br> It is advisable to use this notebook to get practice and debug your code. To speed up the execution, once you are ready, you should move to a scripted version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79b286-de2e-4f79-b04f-f5dc77e50c59",
   "metadata": {},
   "source": [
    "## 1. - Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a0112-75c6-4fd4-887d-0c1fe660c9ac",
   "metadata": {},
   "source": [
    "Select `CUDA_VISIBLE_DEVICES` to the `Graphics Proceesing Unit (GPU)` index that you want to use to enable the use of GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7db3603-a5ec-4fd2-ae28-4571762fc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # GPU index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62227b7f-8c1a-479f-ae34-84611d656702",
   "metadata": {},
   "source": [
    "Enabling autoreload of different packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09a54f6-3fc4-463a-a93f-72cdd4ea75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26166ece-1cb3-4231-b2c6-3df9b39488c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(\"..\", \"data\"))\n",
    "sys.path.insert(1, os.path.join(\"..\", \"utils\"))\n",
    "from torchvision import datasets, transforms\n",
    "from plot_utils import plot_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e285782-59e0-4816-bcf8-bf66981200a2",
   "metadata": {},
   "source": [
    "## 2. - Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7a7a3-d603-4050-9fb4-f1a20c09a072",
   "metadata": {},
   "source": [
    "### 2.1 - Creating datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d110f-1b25-4445-bd84-1a29f9261e7e",
   "metadata": {},
   "source": [
    "Now we read the images from the target directory `path_data`. Set `path_data` to the directory containing the `Cloud`, `Edge`, `Good` subfolders.  Moreover, it will automatically split the total dataset into the train, cross validation and test splits by using a pseudo-random splitting algorithm. You can reproduce the split by specifying the variable `seed`. **NB**:\n",
    "- The train split contains 70% of the whole images.\n",
    "- The valid splits contains 15% of the whole images.\n",
    "- The test splits contains 15% of the whole images.<br>**YOU MUST NOT CHANGE THE TEST SPLIT SIZE!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b8b832-ce8e-4512-9bdb-6c73d715feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data folder (update the variable to your path).\n",
    "path_data=os.path.join(\"..\", \"data\")\n",
    "# Seed value\n",
    "seed=22\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "\n",
    "image_width = 1024\n",
    "image_height = 1942\n",
    "scale_factor = 0.1\n",
    "\n",
    "valid_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "dataset_mean = [0.2391, 0.4028, 0.4096]\n",
    "dataset_std = [0.2312, 0.3223, 0.3203]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((dataset_mean),(dataset_std)),\n",
    "                                transforms.Resize((int(image_width*scale_factor), int(image_height*scale_factor)))])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=path_data, \n",
    "                                 transform=transform)\n",
    "\n",
    "n_val = int(np.floor(valid_size * len(dataset)))\n",
    "n_test = int(np.floor(test_size * len(dataset)))\n",
    "n_train = len(dataset) - n_val - n_test\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c19bcb-6b39-4258-9d5b-157dab7bc253",
   "metadata": {},
   "source": [
    "**Hint:** before proceeding, make sure that your `Edge`,`Cloud`, and `Good` samples are well enough among the `train`, `valid`,`test` splits. To print datasets statistics, run the next line.  Remember that the number of images in the different splits is distributed as described above. <br> If you are not happy with the data distribution, you can update the seed used and create a new dataset by rerunning the cell above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c1e33-6c9d-4020-bb4b-9a95cd7e77fa",
   "metadata": {},
   "source": [
    "### 2.2. - Create data loaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5c989-13e1-48b6-bb99-b09027a94675",
   "metadata": {},
   "source": [
    "The next lines will create a dataloader. A data loader is used to break the dataset into batches of a size `batch_size`. <br> This is useful to ensure that your dataset will fit into your memory and to create a \"stochastic\" implementation of gradient descent. <br> For more information, please, check: [data loader](https://www.educative.io/answers/what-is-pytorch-dataloader).<br>\n",
    "Specify `batch_size` (**Hint**: use powers of 2. Typical values are between 8 and 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e0f8a6d-d620-424d-abae-48789e9c64f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:  {0: 71, 1: 71, 2: 72}\n",
      "Test split:  {0: 13, 1: 22, 2: 10}\n",
      "Validation split:  {0: 12, 1: 9, 2: 24}\n"
     ]
    }
   ],
   "source": [
    "batch_size=16\n",
    "# Train loader\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Cross validation data loader\n",
    "valid_loader = DataLoader(val_ds, batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Test data loader\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "\n",
    "#0 - cloud\n",
    "#1 - edge\n",
    "#2 - good\n",
    "\n",
    "unique, counts = np.unique(torch.tensor([train_ds.dataset.targets[i] for i in train_ds.indices]), return_counts=True)\n",
    "print(\"Train split: \", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(torch.tensor([test_ds.dataset.targets[i] for i in test_ds.indices]), return_counts=True)\n",
    "print(\"Test split: \", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(torch.tensor([val_ds.dataset.targets[i] for i in val_ds.indices]), return_counts=True)\n",
    "print(\"Validation split: \", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec454c-0b6d-411c-bc63-7223de91dabd",
   "metadata": {},
   "source": [
    "## 3 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b7c14-2bf3-47c7-a2ff-ab80c5aa2c0a",
   "metadata": {},
   "source": [
    "Now, it is your turn! Add your code below to load a Neural Network model, select optimizers, learning rate and perform training. <br>\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63443f-23d3-4bb8-aab0-d563092b928d",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97c1aba-ab0e-4d27-b704-6b72c3cca542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\20181212/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU to train model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20181212\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 6.62, Validation Loss: 6.62, accuracy = 0.00\n",
      "Epoch: 2, Training Loss: 6.16, Validation Loss: 6.20, accuracy = 0.07\n",
      "Epoch: 3, Training Loss: 5.73, Validation Loss: 5.77, accuracy = 0.20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 128\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing CPU to train model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 128\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 72\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_fn, train_loader, val_loader, epochs, device)\u001b[0m\n\u001b[0;32m     69\u001b[0m num_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m     70\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 72\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\torchvision\\datasets\\folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    247\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\PIL\\Image.py:911\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    865\u001b[0m ):\n\u001b[0;32m    866\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "    \n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18')\n",
    "    \n",
    "import torch.optim as optim\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0003)\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using GPU to train model\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"using CPU to train model\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train(model, optimizer, torch.nn.CrossEntropyLoss(),train_loader, valid_loader, epochs=500, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edec249-b41d-42f1-8d7c-fa8b9eb63393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test(model, test_loader, loss_fn, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    num_correct = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = num_correct / num_examples\n",
    "    print('Test Loss: {:.2f}, Test Accuracy: {:.2f}'.format(test_loss, accuracy))\n",
    "\n",
    "test(model, test_loader, torch.nn.CrossEntropyLoss(), device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adef2cd-09a3-4a1f-8554-46f97ab63841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

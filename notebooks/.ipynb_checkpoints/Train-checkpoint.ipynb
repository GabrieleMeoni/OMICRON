{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beefc598-b578-4420-ae2a-14ba2ea15a9e",
   "metadata": {},
   "source": [
    "# Train example notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe940bf-d307-4329-a62d-5b3810a293b9",
   "metadata": {},
   "source": [
    "This notebook is used to implement the training of a neural network for classification of `Cloud`, `Edge`, `Good` images. <br> It is advisable to use this notebook to get practice and debug your code. To speed up the execution, once you are ready, you should move to a scripted version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79b286-de2e-4f79-b04f-f5dc77e50c59",
   "metadata": {},
   "source": [
    "## 1. - Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a0112-75c6-4fd4-887d-0c1fe660c9ac",
   "metadata": {},
   "source": [
    "Select `CUDA_VISIBLE_DEVICES` to the `Graphics Proceesing Unit (GPU)` index that you want to use to enable the use of GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7db3603-a5ec-4fd2-ae28-4571762fc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # GPU index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62227b7f-8c1a-479f-ae34-84611d656702",
   "metadata": {},
   "source": [
    "Enabling autoreload of different packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a09a54f6-3fc4-463a-a93f-72cdd4ea75d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26166ece-1cb3-4231-b2c6-3df9b39488c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(\"..\", \"data\"))\n",
    "sys.path.insert(1, os.path.join(\"..\", \"utils\"))\n",
    "from data_utils import Dataset\n",
    "from plot_utils import plot_image\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e285782-59e0-4816-bcf8-bf66981200a2",
   "metadata": {},
   "source": [
    "## 2. - Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7a7a3-d603-4050-9fb4-f1a20c09a072",
   "metadata": {},
   "source": [
    "### 2.1 - Creating datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d110f-1b25-4445-bd84-1a29f9261e7e",
   "metadata": {},
   "source": [
    "Now we read the images from the target directory `path_data`. Set `path_data` to the directory containing the `Cloud`, `Edge`, `Good` subfolders.  Moreover, it will automatically split the total dataset into the train, cross validation and test splits by using a pseudo-random splitting algorithm. You can reproduce the split by specifying the variable `seed`. **NB**:\n",
    "- The train split contains 70% of the whole images.\n",
    "- The valid splits contains 15% of the whole images.\n",
    "- The test splits contains 15% of the whole images.<br>**YOU MUST NOT CHANGE THE TEST SPLIT SIZE!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b8b832-ce8e-4512-9bdb-6c73d715feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data folder (update the variable to your path).\n",
    "path_data=os.path.join(\"..\", \"data\")\n",
    "# Seed value\n",
    "seed=22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2866840-8962-4d55-8461-5819cc496008",
   "metadata": {},
   "source": [
    "<img src=\"utilities/images/danger_icon.png\" style=\"margin:auto\"/>\n",
    "\n",
    "**N.B** Make sure to have created a dataset split into the three directories `Cloud`, and `Good`, `Edge`. Otherwise, the next cell will **fail!** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d16b581c-5f04-4315-b14a-44bad116a22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing class: Cloud: 76it [00:06, 12.03it/s]\n",
      "Parsing class: Edge: 76it [00:05, 14.35it/s]\n",
      "Parsing class: Good: 90it [00:07, 12.27it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset=Dataset(path_data=path_data, seed=seed)\n",
    "dataset.read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c19bcb-6b39-4258-9d5b-157dab7bc253",
   "metadata": {},
   "source": [
    "**Hint:** before proceeding, make sure that your `Edge`,`Cloud`, and `Good` samples are well enough among the `train`, `valid`,`test` splits. To print datasets statistics, run the next line.  Remember that the number of images in the different splits is distributed as described above. <br> If you are not happy with the data distribution, you can update the seed used and create a new dataset by rerunning the cell above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57b0127d-d729-48e7-a1c4-557606850ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>valid</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cloud</th>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edge</th>\n",
       "      <td>51</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>65</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train  valid  test\n",
       "cloud     52      9    15\n",
       "edge      51     13    12\n",
       "good      65     15    10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c1e33-6c9d-4020-bb4b-9a95cd7e77fa",
   "metadata": {},
   "source": [
    "### 2.2. - Create data loaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5c989-13e1-48b6-bb99-b09027a94675",
   "metadata": {},
   "source": [
    "The next lines will create a dataloader. A data loader is used to break the dataset into batches of a size `batch_size`. <br> This is useful to ensure that your dataset will fit into your memory and to create a \"stochastic\" implementation of gradient descent. <br> For more information, please, check: [data loader](https://www.educative.io/answers/what-is-pytorch-dataloader).<br>\n",
    "Specify `batch_size` (**Hint**: use powers of 2. Typical values are between 8 and 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "209d3834-94e0-4965-9a3e-3525211af1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e0f8a6d-d620-424d-abae-48789e9c64f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loader\n",
    "train_loader = DataLoader(dataset.get_split(\"train\"), batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Cross validation data loader\n",
    "valid_loader = DataLoader(dataset.get_split(\"valid\"), batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Test data loader\n",
    "test_loader = DataLoader(dataset.get_split(\"test\"), batch_size=batch_size, pin_memory=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec454c-0b6d-411c-bc63-7223de91dabd",
   "metadata": {},
   "source": [
    "## 3 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b7c14-2bf3-47c7-a2ff-ab80c5aa2c0a",
   "metadata": {},
   "source": [
    "Now, it is your turn! Add your code below to load a Neural Network model, select optimizers, learning rate and perform training. <br>\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63443f-23d3-4bb8-aab0-d563092b928d",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e97c1aba-ab0e-4d27-b704-6b72c3cca542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\20181212/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 6.73, Validation Loss: 6.48, accuracy = 0.00\n",
      "Epoch: 2, Training Loss: 6.34, Validation Loss: 6.10, accuracy = 0.00\n",
      "Epoch: 3, Training Loss: 5.95, Validation Loss: 5.69, accuracy = 0.27\n",
      "Epoch: 4, Training Loss: 5.54, Validation Loss: 5.29, accuracy = 0.51\n",
      "Epoch: 5, Training Loss: 5.14, Validation Loss: 4.87, accuracy = 0.62\n",
      "Epoch: 6, Training Loss: 4.80, Validation Loss: 4.33, accuracy = 0.62\n",
      "Epoch: 7, Training Loss: 4.43, Validation Loss: 4.07, accuracy = 0.51\n",
      "Epoch: 8, Training Loss: 4.04, Validation Loss: 3.66, accuracy = 0.62\n",
      "Epoch: 9, Training Loss: 3.77, Validation Loss: 3.49, accuracy = 0.59\n",
      "Epoch: 10, Training Loss: 3.41, Validation Loss: 3.12, accuracy = 0.65\n",
      "Epoch: 11, Training Loss: 3.18, Validation Loss: 2.89, accuracy = 0.59\n",
      "Epoch: 12, Training Loss: 2.84, Validation Loss: 2.67, accuracy = 0.62\n",
      "Epoch: 13, Training Loss: 2.62, Validation Loss: 2.30, accuracy = 0.68\n",
      "Epoch: 14, Training Loss: 2.37, Validation Loss: 2.15, accuracy = 0.62\n",
      "Epoch: 15, Training Loss: 2.22, Validation Loss: 2.02, accuracy = 0.65\n",
      "Epoch: 16, Training Loss: 2.03, Validation Loss: 1.74, accuracy = 0.70\n",
      "Epoch: 17, Training Loss: 1.88, Validation Loss: 1.61, accuracy = 0.70\n",
      "Epoch: 18, Training Loss: 1.73, Validation Loss: 1.53, accuracy = 0.68\n",
      "Epoch: 19, Training Loss: 1.61, Validation Loss: 1.39, accuracy = 0.68\n",
      "Epoch: 20, Training Loss: 1.55, Validation Loss: 1.20, accuracy = 0.65\n",
      "Epoch: 21, Training Loss: 1.44, Validation Loss: 1.25, accuracy = 0.70\n",
      "Epoch: 22, Training Loss: 1.33, Validation Loss: 1.22, accuracy = 0.73\n",
      "Epoch: 23, Training Loss: 1.37, Validation Loss: 1.15, accuracy = 0.78\n",
      "Epoch: 24, Training Loss: 1.21, Validation Loss: 1.13, accuracy = 0.78\n",
      "Epoch: 25, Training Loss: 1.12, Validation Loss: 1.08, accuracy = 0.76\n",
      "Epoch: 26, Training Loss: 1.17, Validation Loss: 1.05, accuracy = 0.68\n",
      "Epoch: 27, Training Loss: 1.09, Validation Loss: 0.93, accuracy = 0.73\n",
      "Epoch: 28, Training Loss: 1.02, Validation Loss: 1.01, accuracy = 0.76\n",
      "Epoch: 29, Training Loss: 1.02, Validation Loss: 0.89, accuracy = 0.78\n",
      "Epoch: 30, Training Loss: 0.96, Validation Loss: 0.89, accuracy = 0.73\n",
      "Epoch: 31, Training Loss: 1.01, Validation Loss: 0.90, accuracy = 0.73\n",
      "Epoch: 32, Training Loss: 0.96, Validation Loss: 0.83, accuracy = 0.76\n",
      "Epoch: 33, Training Loss: 0.96, Validation Loss: 0.90, accuracy = 0.68\n",
      "Epoch: 34, Training Loss: 0.92, Validation Loss: 0.89, accuracy = 0.68\n",
      "Epoch: 35, Training Loss: 0.89, Validation Loss: 0.81, accuracy = 0.76\n",
      "Epoch: 36, Training Loss: 0.87, Validation Loss: 0.78, accuracy = 0.76\n",
      "Epoch: 37, Training Loss: 0.84, Validation Loss: 0.75, accuracy = 0.76\n",
      "Epoch: 38, Training Loss: 0.87, Validation Loss: 0.76, accuracy = 0.78\n",
      "Epoch: 39, Training Loss: 0.85, Validation Loss: 0.85, accuracy = 0.65\n",
      "Epoch: 40, Training Loss: 0.90, Validation Loss: 0.85, accuracy = 0.81\n",
      "Epoch: 41, Training Loss: 0.79, Validation Loss: 0.80, accuracy = 0.76\n",
      "Epoch: 42, Training Loss: 0.83, Validation Loss: 0.76, accuracy = 0.78\n",
      "Epoch: 43, Training Loss: 0.81, Validation Loss: 0.80, accuracy = 0.65\n",
      "Epoch: 44, Training Loss: 0.81, Validation Loss: 0.72, accuracy = 0.76\n",
      "Epoch: 45, Training Loss: 0.74, Validation Loss: 0.73, accuracy = 0.78\n",
      "Epoch: 46, Training Loss: 0.75, Validation Loss: 0.76, accuracy = 0.68\n",
      "Epoch: 47, Training Loss: 0.82, Validation Loss: 0.77, accuracy = 0.68\n",
      "Epoch: 48, Training Loss: 0.75, Validation Loss: 0.71, accuracy = 0.78\n",
      "Epoch: 49, Training Loss: 0.74, Validation Loss: 0.72, accuracy = 0.76\n",
      "Epoch: 50, Training Loss: 0.79, Validation Loss: 0.69, accuracy = 0.78\n",
      "Epoch: 51, Training Loss: 0.72, Validation Loss: 0.69, accuracy = 0.73\n",
      "Epoch: 52, Training Loss: 0.73, Validation Loss: 0.74, accuracy = 0.76\n",
      "Epoch: 53, Training Loss: 0.73, Validation Loss: 0.74, accuracy = 0.68\n",
      "Epoch: 54, Training Loss: 0.78, Validation Loss: 0.73, accuracy = 0.73\n",
      "Epoch: 55, Training Loss: 0.71, Validation Loss: 0.70, accuracy = 0.81\n",
      "Epoch: 56, Training Loss: 0.71, Validation Loss: 0.70, accuracy = 0.78\n",
      "Epoch: 57, Training Loss: 0.69, Validation Loss: 0.65, accuracy = 0.78\n",
      "Epoch: 58, Training Loss: 0.75, Validation Loss: 0.67, accuracy = 0.73\n",
      "Epoch: 59, Training Loss: 0.72, Validation Loss: 0.65, accuracy = 0.73\n",
      "Epoch: 60, Training Loss: 0.70, Validation Loss: 0.72, accuracy = 0.70\n",
      "Epoch: 61, Training Loss: 0.68, Validation Loss: 0.65, accuracy = 0.81\n",
      "Epoch: 62, Training Loss: 0.65, Validation Loss: 0.71, accuracy = 0.73\n",
      "Epoch: 63, Training Loss: 0.70, Validation Loss: 0.63, accuracy = 0.84\n",
      "Epoch: 64, Training Loss: 0.66, Validation Loss: 0.64, accuracy = 0.78\n",
      "Epoch: 65, Training Loss: 0.66, Validation Loss: 0.63, accuracy = 0.73\n",
      "Epoch: 66, Training Loss: 0.67, Validation Loss: 0.72, accuracy = 0.73\n",
      "Epoch: 67, Training Loss: 0.66, Validation Loss: 0.60, accuracy = 0.78\n",
      "Epoch: 68, Training Loss: 0.65, Validation Loss: 0.64, accuracy = 0.78\n",
      "Epoch: 69, Training Loss: 0.62, Validation Loss: 0.69, accuracy = 0.76\n",
      "Epoch: 70, Training Loss: 0.66, Validation Loss: 0.66, accuracy = 0.70\n",
      "Epoch: 71, Training Loss: 0.65, Validation Loss: 0.63, accuracy = 0.73\n",
      "Epoch: 72, Training Loss: 0.60, Validation Loss: 0.71, accuracy = 0.70\n",
      "Epoch: 73, Training Loss: 0.64, Validation Loss: 0.64, accuracy = 0.76\n",
      "Epoch: 74, Training Loss: 0.60, Validation Loss: 0.69, accuracy = 0.76\n",
      "Epoch: 75, Training Loss: 0.69, Validation Loss: 0.65, accuracy = 0.78\n",
      "Epoch: 76, Training Loss: 0.63, Validation Loss: 0.64, accuracy = 0.76\n",
      "Epoch: 77, Training Loss: 0.69, Validation Loss: 0.62, accuracy = 0.76\n",
      "Epoch: 78, Training Loss: 0.65, Validation Loss: 0.59, accuracy = 0.76\n",
      "Epoch: 79, Training Loss: 0.69, Validation Loss: 0.55, accuracy = 0.76\n",
      "Epoch: 80, Training Loss: 0.62, Validation Loss: 0.57, accuracy = 0.78\n",
      "Epoch: 81, Training Loss: 0.66, Validation Loss: 0.61, accuracy = 0.76\n",
      "Epoch: 82, Training Loss: 0.61, Validation Loss: 0.68, accuracy = 0.70\n",
      "Epoch: 83, Training Loss: 0.60, Validation Loss: 0.62, accuracy = 0.81\n",
      "Epoch: 84, Training Loss: 0.62, Validation Loss: 0.61, accuracy = 0.78\n",
      "Epoch: 85, Training Loss: 0.63, Validation Loss: 0.59, accuracy = 0.78\n",
      "Epoch: 86, Training Loss: 0.61, Validation Loss: 0.56, accuracy = 0.78\n",
      "Epoch: 87, Training Loss: 0.65, Validation Loss: 0.64, accuracy = 0.78\n",
      "Epoch: 88, Training Loss: 0.60, Validation Loss: 0.60, accuracy = 0.81\n",
      "Epoch: 89, Training Loss: 0.58, Validation Loss: 0.58, accuracy = 0.81\n",
      "Epoch: 90, Training Loss: 0.63, Validation Loss: 0.59, accuracy = 0.78\n",
      "Epoch: 91, Training Loss: 0.61, Validation Loss: 0.62, accuracy = 0.76\n",
      "Epoch: 92, Training Loss: 0.61, Validation Loss: 0.60, accuracy = 0.78\n",
      "Epoch: 93, Training Loss: 0.61, Validation Loss: 0.54, accuracy = 0.81\n",
      "Epoch: 94, Training Loss: 0.59, Validation Loss: 0.57, accuracy = 0.84\n",
      "Epoch: 95, Training Loss: 0.61, Validation Loss: 0.55, accuracy = 0.84\n",
      "Epoch: 96, Training Loss: 0.66, Validation Loss: 0.57, accuracy = 0.76\n",
      "Epoch: 97, Training Loss: 0.66, Validation Loss: 0.63, accuracy = 0.78\n",
      "Epoch: 98, Training Loss: 0.63, Validation Loss: 0.60, accuracy = 0.78\n",
      "Epoch: 99, Training Loss: 0.56, Validation Loss: 0.61, accuracy = 0.76\n",
      "Epoch: 100, Training Loss: 0.59, Validation Loss: 0.72, accuracy = 0.76\n",
      "Epoch: 101, Training Loss: 0.57, Validation Loss: 0.59, accuracy = 0.81\n",
      "Epoch: 102, Training Loss: 0.69, Validation Loss: 0.55, accuracy = 0.78\n",
      "Epoch: 103, Training Loss: 0.58, Validation Loss: 0.77, accuracy = 0.68\n",
      "Epoch: 104, Training Loss: 0.56, Validation Loss: 0.56, accuracy = 0.81\n",
      "Epoch: 105, Training Loss: 0.58, Validation Loss: 0.53, accuracy = 0.76\n",
      "Epoch: 106, Training Loss: 0.56, Validation Loss: 0.51, accuracy = 0.81\n",
      "Epoch: 107, Training Loss: 0.61, Validation Loss: 0.54, accuracy = 0.78\n",
      "Epoch: 108, Training Loss: 0.58, Validation Loss: 0.62, accuracy = 0.81\n",
      "Epoch: 109, Training Loss: 0.58, Validation Loss: 0.67, accuracy = 0.76\n",
      "Epoch: 110, Training Loss: 0.66, Validation Loss: 0.63, accuracy = 0.73\n",
      "Epoch: 111, Training Loss: 0.54, Validation Loss: 0.54, accuracy = 0.81\n",
      "Epoch: 112, Training Loss: 0.52, Validation Loss: 0.59, accuracy = 0.78\n",
      "Epoch: 113, Training Loss: 0.53, Validation Loss: 0.54, accuracy = 0.81\n",
      "Epoch: 114, Training Loss: 0.61, Validation Loss: 0.53, accuracy = 0.78\n",
      "Epoch: 115, Training Loss: 0.58, Validation Loss: 0.68, accuracy = 0.70\n",
      "Epoch: 116, Training Loss: 0.54, Validation Loss: 0.59, accuracy = 0.81\n",
      "Epoch: 117, Training Loss: 0.54, Validation Loss: 0.59, accuracy = 0.78\n",
      "Epoch: 118, Training Loss: 0.57, Validation Loss: 0.59, accuracy = 0.84\n",
      "Epoch: 119, Training Loss: 0.52, Validation Loss: 0.55, accuracy = 0.76\n",
      "Epoch: 120, Training Loss: 0.54, Validation Loss: 0.56, accuracy = 0.78\n",
      "Epoch: 121, Training Loss: 0.60, Validation Loss: 0.67, accuracy = 0.70\n",
      "Epoch: 122, Training Loss: 0.55, Validation Loss: 0.51, accuracy = 0.84\n",
      "Epoch: 123, Training Loss: 0.54, Validation Loss: 0.55, accuracy = 0.73\n",
      "Epoch: 124, Training Loss: 0.60, Validation Loss: 0.53, accuracy = 0.84\n",
      "Epoch: 125, Training Loss: 0.51, Validation Loss: 0.56, accuracy = 0.78\n",
      "Epoch: 126, Training Loss: 0.50, Validation Loss: 0.59, accuracy = 0.70\n",
      "Epoch: 127, Training Loss: 0.59, Validation Loss: 0.55, accuracy = 0.70\n",
      "Epoch: 128, Training Loss: 0.52, Validation Loss: 0.63, accuracy = 0.70\n",
      "Epoch: 129, Training Loss: 0.54, Validation Loss: 0.59, accuracy = 0.70\n",
      "Epoch: 130, Training Loss: 0.55, Validation Loss: 0.54, accuracy = 0.84\n",
      "Epoch: 131, Training Loss: 0.61, Validation Loss: 0.68, accuracy = 0.78\n",
      "Epoch: 132, Training Loss: 0.57, Validation Loss: 0.56, accuracy = 0.76\n",
      "Epoch: 133, Training Loss: 0.53, Validation Loss: 0.51, accuracy = 0.84\n",
      "Epoch: 134, Training Loss: 0.55, Validation Loss: 0.53, accuracy = 0.78\n",
      "Epoch: 135, Training Loss: 0.52, Validation Loss: 0.48, accuracy = 0.78\n",
      "Epoch: 136, Training Loss: 0.56, Validation Loss: 0.53, accuracy = 0.78\n",
      "Epoch: 137, Training Loss: 0.59, Validation Loss: 0.48, accuracy = 0.81\n",
      "Epoch: 138, Training Loss: 0.50, Validation Loss: 0.46, accuracy = 0.81\n",
      "Epoch: 139, Training Loss: 0.51, Validation Loss: 0.51, accuracy = 0.81\n",
      "Epoch: 140, Training Loss: 0.53, Validation Loss: 0.53, accuracy = 0.76\n",
      "Epoch: 141, Training Loss: 0.52, Validation Loss: 0.51, accuracy = 0.81\n",
      "Epoch: 142, Training Loss: 0.49, Validation Loss: 0.60, accuracy = 0.84\n",
      "Epoch: 143, Training Loss: 0.50, Validation Loss: 0.47, accuracy = 0.81\n",
      "Epoch: 144, Training Loss: 0.49, Validation Loss: 0.55, accuracy = 0.73\n",
      "Epoch: 145, Training Loss: 0.49, Validation Loss: 0.51, accuracy = 0.81\n",
      "Epoch: 146, Training Loss: 0.53, Validation Loss: 0.48, accuracy = 0.81\n",
      "Epoch: 147, Training Loss: 0.52, Validation Loss: 0.58, accuracy = 0.84\n",
      "Epoch: 148, Training Loss: 0.50, Validation Loss: 0.52, accuracy = 0.78\n",
      "Epoch: 149, Training Loss: 0.47, Validation Loss: 0.52, accuracy = 0.81\n",
      "Epoch: 150, Training Loss: 0.51, Validation Loss: 0.50, accuracy = 0.81\n",
      "Epoch: 151, Training Loss: 0.56, Validation Loss: 0.49, accuracy = 0.81\n",
      "Epoch: 152, Training Loss: 0.49, Validation Loss: 0.49, accuracy = 0.84\n",
      "Epoch: 153, Training Loss: 0.56, Validation Loss: 0.54, accuracy = 0.81\n",
      "Epoch: 154, Training Loss: 0.50, Validation Loss: 0.56, accuracy = 0.81\n",
      "Epoch: 155, Training Loss: 0.52, Validation Loss: 0.49, accuracy = 0.76\n",
      "Epoch: 156, Training Loss: 0.53, Validation Loss: 0.54, accuracy = 0.78\n",
      "Epoch: 157, Training Loss: 0.51, Validation Loss: 0.61, accuracy = 0.81\n",
      "Epoch: 158, Training Loss: 0.54, Validation Loss: 0.53, accuracy = 0.78\n",
      "Epoch: 159, Training Loss: 0.55, Validation Loss: 0.63, accuracy = 0.76\n",
      "Epoch: 160, Training Loss: 0.52, Validation Loss: 0.67, accuracy = 0.76\n",
      "Epoch: 161, Training Loss: 0.50, Validation Loss: 0.49, accuracy = 0.81\n",
      "Epoch: 162, Training Loss: 0.52, Validation Loss: 0.49, accuracy = 0.84\n",
      "Epoch: 163, Training Loss: 0.54, Validation Loss: 0.46, accuracy = 0.81\n",
      "Epoch: 164, Training Loss: 0.49, Validation Loss: 0.64, accuracy = 0.70\n",
      "Epoch: 165, Training Loss: 0.56, Validation Loss: 0.45, accuracy = 0.78\n",
      "Epoch: 166, Training Loss: 0.46, Validation Loss: 0.46, accuracy = 0.81\n",
      "Epoch: 167, Training Loss: 0.53, Validation Loss: 0.57, accuracy = 0.73\n",
      "Epoch: 168, Training Loss: 0.48, Validation Loss: 0.53, accuracy = 0.73\n",
      "Epoch: 169, Training Loss: 0.46, Validation Loss: 0.62, accuracy = 0.84\n",
      "Epoch: 170, Training Loss: 0.50, Validation Loss: 0.61, accuracy = 0.73\n",
      "Epoch: 171, Training Loss: 0.47, Validation Loss: 0.48, accuracy = 0.81\n",
      "Epoch: 172, Training Loss: 0.46, Validation Loss: 0.48, accuracy = 0.89\n",
      "Epoch: 173, Training Loss: 0.46, Validation Loss: 0.48, accuracy = 0.81\n",
      "Epoch: 174, Training Loss: 0.44, Validation Loss: 0.50, accuracy = 0.76\n",
      "Epoch: 175, Training Loss: 0.52, Validation Loss: 0.73, accuracy = 0.68\n",
      "Epoch: 176, Training Loss: 0.49, Validation Loss: 0.60, accuracy = 0.76\n",
      "Epoch: 177, Training Loss: 0.53, Validation Loss: 0.65, accuracy = 0.70\n",
      "Epoch: 178, Training Loss: 0.47, Validation Loss: 0.48, accuracy = 0.78\n",
      "Epoch: 179, Training Loss: 0.50, Validation Loss: 0.54, accuracy = 0.76\n",
      "Epoch: 180, Training Loss: 0.43, Validation Loss: 0.51, accuracy = 0.81\n",
      "Epoch: 181, Training Loss: 0.45, Validation Loss: 0.46, accuracy = 0.81\n",
      "Epoch: 182, Training Loss: 0.49, Validation Loss: 0.55, accuracy = 0.81\n",
      "Epoch: 183, Training Loss: 0.48, Validation Loss: 0.61, accuracy = 0.81\n",
      "Epoch: 184, Training Loss: 0.50, Validation Loss: 0.52, accuracy = 0.81\n",
      "Epoch: 185, Training Loss: 0.51, Validation Loss: 0.46, accuracy = 0.84\n",
      "Epoch: 186, Training Loss: 0.50, Validation Loss: 0.46, accuracy = 0.81\n",
      "Epoch: 187, Training Loss: 0.47, Validation Loss: 0.52, accuracy = 0.81\n",
      "Epoch: 188, Training Loss: 0.45, Validation Loss: 0.54, accuracy = 0.76\n",
      "Epoch: 189, Training Loss: 0.47, Validation Loss: 0.51, accuracy = 0.73\n",
      "Epoch: 190, Training Loss: 0.46, Validation Loss: 0.49, accuracy = 0.76\n",
      "Epoch: 191, Training Loss: 0.45, Validation Loss: 0.55, accuracy = 0.78\n",
      "Epoch: 192, Training Loss: 0.50, Validation Loss: 0.50, accuracy = 0.73\n",
      "Epoch: 193, Training Loss: 0.44, Validation Loss: 0.49, accuracy = 0.78\n",
      "Epoch: 194, Training Loss: 0.49, Validation Loss: 0.54, accuracy = 0.81\n",
      "Epoch: 195, Training Loss: 0.44, Validation Loss: 0.49, accuracy = 0.81\n",
      "Epoch: 196, Training Loss: 0.44, Validation Loss: 0.49, accuracy = 0.81\n",
      "Epoch: 197, Training Loss: 0.47, Validation Loss: 0.54, accuracy = 0.84\n",
      "Epoch: 198, Training Loss: 0.49, Validation Loss: 0.61, accuracy = 0.81\n",
      "Epoch: 199, Training Loss: 0.42, Validation Loss: 0.50, accuracy = 0.81\n",
      "Epoch: 200, Training Loss: 0.44, Validation Loss: 0.48, accuracy = 0.78\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "    \n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18')\n",
    "    \n",
    "import torch.optim as optim\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0003)\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=100, device=\"cpu\"):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train(model, optimizer, torch.nn.CrossEntropyLoss(),train_loader, valid_loader, epochs=200, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9edec249-b41d-42f1-8d7c-fa8b9eb63393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.77, Test Accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "# Function to test the model\n",
    "def test(model, test_loader, loss_fn, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    num_correct = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = num_correct / num_examples\n",
    "    print('Test Loss: {:.2f}, Test Accuracy: {:.2f}'.format(test_loss, accuracy))\n",
    "\n",
    "test(model, test_loader, torch.nn.CrossEntropyLoss(), device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adef2cd-09a3-4a1f-8554-46f97ab63841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beefc598-b578-4420-ae2a-14ba2ea15a9e",
   "metadata": {},
   "source": [
    "# Train example notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe940bf-d307-4329-a62d-5b3810a293b9",
   "metadata": {},
   "source": [
    "This notebook is used to implement the training of a neural network for classification of `Cloud`, `Edge`, `Good` images. <br> It is advisable to use this notebook to get practice and debug your code. To speed up the execution, once you are ready, you should move to a scripted version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79b286-de2e-4f79-b04f-f5dc77e50c59",
   "metadata": {},
   "source": [
    "## 1. - Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a0112-75c6-4fd4-887d-0c1fe660c9ac",
   "metadata": {},
   "source": [
    "Select `CUDA_VISIBLE_DEVICES` to the `Graphics Proceesing Unit (GPU)` index that you want to use to enable the use of GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7db3603-a5ec-4fd2-ae28-4571762fc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # GPU index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62227b7f-8c1a-479f-ae34-84611d656702",
   "metadata": {},
   "source": [
    "Enabling autoreload of different packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09a54f6-3fc4-463a-a93f-72cdd4ea75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26166ece-1cb3-4231-b2c6-3df9b39488c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(\"..\", \"data\"))\n",
    "sys.path.insert(1, os.path.join(\"..\", \"utils\"))\n",
    "from data_utils import Dataset\n",
    "from plot_utils import plot_image\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e285782-59e0-4816-bcf8-bf66981200a2",
   "metadata": {},
   "source": [
    "## 2. - Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7a7a3-d603-4050-9fb4-f1a20c09a072",
   "metadata": {},
   "source": [
    "### 2.1 - Creating datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d110f-1b25-4445-bd84-1a29f9261e7e",
   "metadata": {},
   "source": [
    "Now we read the images from the target directory `path_data`. Set `path_data` to the directory containing the `Cloud`, `Edge`, `Good` subfolders.  Moreover, it will automatically split the total dataset into the train, cross validation and test splits by using a pseudo-random splitting algorithm. You can reproduce the split by specifying the variable `seed`. **NB**:\n",
    "- The train split contains 70% of the whole images.\n",
    "- The valid splits contains 15% of the whole images.\n",
    "- The test splits contains 15% of the whole images.<br>**YOU MUST NOT CHANGE THE TEST SPLIT SIZE!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b8b832-ce8e-4512-9bdb-6c73d715feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data folder (update the variable to your path).\n",
    "path_data=os.path.join(\"..\", \"data\")\n",
    "# Seed value\n",
    "seed=22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2866840-8962-4d55-8461-5819cc496008",
   "metadata": {},
   "source": [
    "<img src=\"utilities/images/danger_icon.png\" style=\"margin:auto\"/>\n",
    "\n",
    "**N.B** Make sure to have created a dataset split into the three directories `Cloud`, and `Good`, `Edge`. Otherwise, the next cell will **fail!** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16b581c-5f04-4315-b14a-44bad116a22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing class: Cloud: 96it [00:10,  8.75it/s]\n",
      "Parsing class: Edge: 102it [00:08, 11.40it/s]\n",
      "Parsing class: Good: 106it [00:10,  9.74it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset=Dataset(path_data=path_data, seed=seed)\n",
    "dataset.read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c19bcb-6b39-4258-9d5b-157dab7bc253",
   "metadata": {},
   "source": [
    "**Hint:** before proceeding, make sure that your `Edge`,`Cloud`, and `Good` samples are well enough among the `train`, `valid`,`test` splits. To print datasets statistics, run the next line.  Remember that the number of images in the different splits is distributed as described above. <br> If you are not happy with the data distribution, you can update the seed used and create a new dataset by rerunning the cell above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57b0127d-d729-48e7-a1c4-557606850ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>valid</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cloud</th>\n",
       "      <td>68</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edge</th>\n",
       "      <td>77</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train  valid  test\n",
       "cloud     68     15    13\n",
       "edge      77     13    12\n",
       "good      67     18    21"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c1e33-6c9d-4020-bb4b-9a95cd7e77fa",
   "metadata": {},
   "source": [
    "### 2.2. - Create data loaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5c989-13e1-48b6-bb99-b09027a94675",
   "metadata": {},
   "source": [
    "The next lines will create a dataloader. A data loader is used to break the dataset into batches of a size `batch_size`. <br> This is useful to ensure that your dataset will fit into your memory and to create a \"stochastic\" implementation of gradient descent. <br> For more information, please, check: [data loader](https://www.educative.io/answers/what-is-pytorch-dataloader).<br>\n",
    "Specify `batch_size` (**Hint**: use powers of 2. Typical values are between 8 and 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "209d3834-94e0-4965-9a3e-3525211af1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e0f8a6d-d620-424d-abae-48789e9c64f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loader\n",
    "train_loader = DataLoader(dataset.get_split(\"train\"), batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Cross validation data loader\n",
    "valid_loader = DataLoader(dataset.get_split(\"valid\"), batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Test data loader\n",
    "test_loader = DataLoader(dataset.get_split(\"test\"), batch_size=batch_size, pin_memory=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec454c-0b6d-411c-bc63-7223de91dabd",
   "metadata": {},
   "source": [
    "## 3 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b7c14-2bf3-47c7-a2ff-ab80c5aa2c0a",
   "metadata": {},
   "source": [
    "Now, it is your turn! Add your code below to load a Neural Network model, select optimizers, learning rate and perform training. <br>\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63443f-23d3-4bb8-aab0-d563092b928d",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e97c1aba-ab0e-4d27-b704-6b72c3cca542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to C:\\Users\\sebas/.cache\\torch\\hub\\v0.10.0.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 6.75, Validation Loss: 6.52, accuracy = 0.00\n",
      "Epoch: 2, Training Loss: 6.25, Validation Loss: 5.93, accuracy = 0.09\n",
      "Epoch: 3, Training Loss: 5.78, Validation Loss: 5.22, accuracy = 0.39\n",
      "Epoch: 4, Training Loss: 5.31, Validation Loss: 4.92, accuracy = 0.46\n",
      "Epoch: 5, Training Loss: 4.83, Validation Loss: 4.19, accuracy = 0.61\n",
      "Epoch: 6, Training Loss: 4.38, Validation Loss: 3.55, accuracy = 0.65\n",
      "Epoch: 7, Training Loss: 3.92, Validation Loss: 3.41, accuracy = 0.54\n",
      "Epoch: 8, Training Loss: 3.47, Validation Loss: 3.19, accuracy = 0.48\n",
      "Epoch: 9, Training Loss: 3.05, Validation Loss: 2.89, accuracy = 0.43\n",
      "Epoch: 10, Training Loss: 2.74, Validation Loss: 2.35, accuracy = 0.54\n",
      "Epoch: 11, Training Loss: 2.42, Validation Loss: 2.24, accuracy = 0.46\n",
      "Epoch: 12, Training Loss: 2.15, Validation Loss: 1.80, accuracy = 0.48\n",
      "Epoch: 13, Training Loss: 1.93, Validation Loss: 1.79, accuracy = 0.57\n",
      "Epoch: 14, Training Loss: 1.77, Validation Loss: 1.69, accuracy = 0.50\n",
      "Epoch: 15, Training Loss: 1.63, Validation Loss: 1.39, accuracy = 0.65\n",
      "Epoch: 16, Training Loss: 1.52, Validation Loss: 1.50, accuracy = 0.54\n",
      "Epoch: 17, Training Loss: 1.39, Validation Loss: 1.31, accuracy = 0.61\n",
      "Epoch: 18, Training Loss: 1.31, Validation Loss: 1.28, accuracy = 0.57\n",
      "Epoch: 19, Training Loss: 1.29, Validation Loss: 1.19, accuracy = 0.61\n",
      "Epoch: 20, Training Loss: 1.19, Validation Loss: 1.15, accuracy = 0.57\n",
      "Epoch: 21, Training Loss: 1.19, Validation Loss: 1.14, accuracy = 0.57\n",
      "Epoch: 22, Training Loss: 1.16, Validation Loss: 1.05, accuracy = 0.67\n",
      "Epoch: 23, Training Loss: 1.11, Validation Loss: 1.08, accuracy = 0.59\n",
      "Epoch: 24, Training Loss: 1.10, Validation Loss: 1.12, accuracy = 0.54\n",
      "Epoch: 25, Training Loss: 1.08, Validation Loss: 1.15, accuracy = 0.50\n",
      "Epoch: 26, Training Loss: 1.09, Validation Loss: 1.16, accuracy = 0.50\n",
      "Epoch: 27, Training Loss: 1.05, Validation Loss: 1.05, accuracy = 0.59\n",
      "Epoch: 28, Training Loss: 1.09, Validation Loss: 1.20, accuracy = 0.50\n",
      "Epoch: 29, Training Loss: 0.99, Validation Loss: 1.00, accuracy = 0.63\n",
      "Epoch: 30, Training Loss: 1.03, Validation Loss: 1.03, accuracy = 0.57\n",
      "Epoch: 31, Training Loss: 1.00, Validation Loss: 1.01, accuracy = 0.63\n",
      "Epoch: 32, Training Loss: 0.93, Validation Loss: 0.96, accuracy = 0.63\n",
      "Epoch: 33, Training Loss: 0.97, Validation Loss: 1.06, accuracy = 0.54\n",
      "Epoch: 34, Training Loss: 0.93, Validation Loss: 0.92, accuracy = 0.63\n",
      "Epoch: 35, Training Loss: 0.96, Validation Loss: 0.91, accuracy = 0.61\n",
      "Epoch: 36, Training Loss: 0.93, Validation Loss: 0.97, accuracy = 0.54\n",
      "Epoch: 37, Training Loss: 0.88, Validation Loss: 0.92, accuracy = 0.63\n",
      "Epoch: 38, Training Loss: 0.98, Validation Loss: 0.90, accuracy = 0.63\n",
      "Epoch: 39, Training Loss: 0.88, Validation Loss: 0.91, accuracy = 0.61\n",
      "Epoch: 40, Training Loss: 0.95, Validation Loss: 0.89, accuracy = 0.59\n",
      "Epoch: 41, Training Loss: 0.94, Validation Loss: 0.86, accuracy = 0.61\n",
      "Epoch: 42, Training Loss: 0.86, Validation Loss: 0.86, accuracy = 0.65\n",
      "Epoch: 43, Training Loss: 0.89, Validation Loss: 0.94, accuracy = 0.59\n",
      "Epoch: 44, Training Loss: 0.88, Validation Loss: 0.85, accuracy = 0.67\n",
      "Epoch: 45, Training Loss: 0.89, Validation Loss: 0.87, accuracy = 0.65\n",
      "Epoch: 46, Training Loss: 0.84, Validation Loss: 0.89, accuracy = 0.63\n",
      "Epoch: 47, Training Loss: 0.84, Validation Loss: 0.90, accuracy = 0.63\n",
      "Epoch: 48, Training Loss: 0.85, Validation Loss: 0.83, accuracy = 0.65\n",
      "Epoch: 49, Training Loss: 0.84, Validation Loss: 0.86, accuracy = 0.70\n",
      "Epoch: 50, Training Loss: 0.83, Validation Loss: 0.91, accuracy = 0.59\n",
      "Epoch: 51, Training Loss: 0.84, Validation Loss: 0.81, accuracy = 0.65\n",
      "Epoch: 52, Training Loss: 0.85, Validation Loss: 0.87, accuracy = 0.61\n",
      "Epoch: 53, Training Loss: 0.83, Validation Loss: 0.89, accuracy = 0.63\n",
      "Epoch: 54, Training Loss: 0.83, Validation Loss: 0.92, accuracy = 0.57\n",
      "Epoch: 55, Training Loss: 0.90, Validation Loss: 1.05, accuracy = 0.54\n",
      "Epoch: 56, Training Loss: 0.82, Validation Loss: 0.82, accuracy = 0.63\n",
      "Epoch: 57, Training Loss: 0.80, Validation Loss: 0.80, accuracy = 0.65\n",
      "Epoch: 58, Training Loss: 0.84, Validation Loss: 0.82, accuracy = 0.70\n",
      "Epoch: 59, Training Loss: 0.81, Validation Loss: 0.81, accuracy = 0.70\n",
      "Epoch: 60, Training Loss: 0.84, Validation Loss: 0.81, accuracy = 0.67\n",
      "Epoch: 61, Training Loss: 0.81, Validation Loss: 0.82, accuracy = 0.70\n",
      "Epoch: 62, Training Loss: 0.76, Validation Loss: 0.82, accuracy = 0.65\n",
      "Epoch: 63, Training Loss: 0.83, Validation Loss: 0.81, accuracy = 0.67\n",
      "Epoch: 64, Training Loss: 0.79, Validation Loss: 0.83, accuracy = 0.65\n",
      "Epoch: 65, Training Loss: 0.79, Validation Loss: 0.82, accuracy = 0.63\n",
      "Epoch: 66, Training Loss: 0.81, Validation Loss: 0.87, accuracy = 0.63\n",
      "Epoch: 67, Training Loss: 0.78, Validation Loss: 0.77, accuracy = 0.70\n",
      "Epoch: 68, Training Loss: 0.81, Validation Loss: 0.79, accuracy = 0.65\n",
      "Epoch: 69, Training Loss: 0.78, Validation Loss: 0.82, accuracy = 0.63\n",
      "Epoch: 70, Training Loss: 0.75, Validation Loss: 0.83, accuracy = 0.67\n",
      "Epoch: 71, Training Loss: 0.77, Validation Loss: 0.90, accuracy = 0.59\n",
      "Epoch: 72, Training Loss: 0.76, Validation Loss: 0.80, accuracy = 0.67\n",
      "Epoch: 73, Training Loss: 0.79, Validation Loss: 0.81, accuracy = 0.67\n",
      "Epoch: 74, Training Loss: 0.81, Validation Loss: 0.82, accuracy = 0.70\n",
      "Epoch: 75, Training Loss: 0.77, Validation Loss: 0.78, accuracy = 0.65\n",
      "Epoch: 76, Training Loss: 0.74, Validation Loss: 0.80, accuracy = 0.61\n",
      "Epoch: 77, Training Loss: 0.75, Validation Loss: 0.81, accuracy = 0.70\n",
      "Epoch: 78, Training Loss: 0.76, Validation Loss: 0.81, accuracy = 0.61\n",
      "Epoch: 79, Training Loss: 0.77, Validation Loss: 0.80, accuracy = 0.67\n",
      "Epoch: 80, Training Loss: 0.76, Validation Loss: 0.78, accuracy = 0.70\n",
      "Epoch: 81, Training Loss: 0.72, Validation Loss: 0.78, accuracy = 0.70\n",
      "Epoch: 82, Training Loss: 0.74, Validation Loss: 0.81, accuracy = 0.67\n",
      "Epoch: 83, Training Loss: 0.72, Validation Loss: 0.75, accuracy = 0.67\n",
      "Epoch: 84, Training Loss: 0.76, Validation Loss: 0.79, accuracy = 0.65\n",
      "Epoch: 85, Training Loss: 0.76, Validation Loss: 0.80, accuracy = 0.67\n",
      "Epoch: 86, Training Loss: 0.75, Validation Loss: 0.75, accuracy = 0.72\n",
      "Epoch: 87, Training Loss: 0.69, Validation Loss: 0.79, accuracy = 0.65\n",
      "Epoch: 88, Training Loss: 0.77, Validation Loss: 0.76, accuracy = 0.63\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 59\u001b[0m\n\u001b[0;32m     55\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 59\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_fn, train_loader, val_loader, epochs, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 30\u001b[0m     training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     31\u001b[0m training_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "    \n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18')\n",
    "    \n",
    "import torch.optim as optim\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0003)\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=100, device=\"cpu\"):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train(model, optimizer, torch.nn.CrossEntropyLoss(),train_loader, valid_loader, epochs=500, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edec249-b41d-42f1-8d7c-fa8b9eb63393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.77, Test Accuracy: 0.57\n"
     ]
    }
   ],
   "source": [
    "# Function to test the model\n",
    "def test(model, test_loader, loss_fn, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    num_correct = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = num_correct / num_examples\n",
    "    print('Test Loss: {:.2f}, Test Accuracy: {:.2f}'.format(test_loss, accuracy))\n",
    "\n",
    "test(model, test_loader, torch.nn.CrossEntropyLoss(), device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adef2cd-09a3-4a1f-8554-46f97ab63841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

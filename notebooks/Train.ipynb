{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beefc598-b578-4420-ae2a-14ba2ea15a9e",
   "metadata": {},
   "source": [
    "# Train example notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe940bf-d307-4329-a62d-5b3810a293b9",
   "metadata": {},
   "source": [
    "This notebook is used to implement the training of a neural network for classification of `Cloud`, `Edge`, `Good` images. <br> It is advisable to use this notebook to get practice and debug your code. To speed up the execution, once you are ready, you should move to a scripted version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79b286-de2e-4f79-b04f-f5dc77e50c59",
   "metadata": {},
   "source": [
    "## 1. - Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a0112-75c6-4fd4-887d-0c1fe660c9ac",
   "metadata": {},
   "source": [
    "Select `CUDA_VISIBLE_DEVICES` to the `Graphics Proceesing Unit (GPU)` index that you want to use to enable the use of GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7db3603-a5ec-4fd2-ae28-4571762fc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # GPU index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62227b7f-8c1a-479f-ae34-84611d656702",
   "metadata": {},
   "source": [
    "Enabling autoreload of different packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09a54f6-3fc4-463a-a93f-72cdd4ea75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26166ece-1cb3-4231-b2c6-3df9b39488c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(\"..\", \"data\"))\n",
    "sys.path.insert(1, os.path.join(\"..\", \"utils\"))\n",
    "from torchvision import datasets, transforms\n",
    "from plot_utils import plot_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e285782-59e0-4816-bcf8-bf66981200a2",
   "metadata": {},
   "source": [
    "## 2. - Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7a7a3-d603-4050-9fb4-f1a20c09a072",
   "metadata": {},
   "source": [
    "### 2.1 - Creating datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d110f-1b25-4445-bd84-1a29f9261e7e",
   "metadata": {},
   "source": [
    "Now we read the images from the target directory `path_data`. Set `path_data` to the directory containing the `Cloud`, `Edge`, `Good` subfolders.  Moreover, it will automatically split the total dataset into the train, cross validation and test splits by using a pseudo-random splitting algorithm. You can reproduce the split by specifying the variable `seed`. **NB**:\n",
    "- The train split contains 70% of the whole images.\n",
    "- The valid splits contains 15% of the whole images.\n",
    "- The test splits contains 15% of the whole images.<br>**YOU MUST NOT CHANGE THE TEST SPLIT SIZE!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b8b832-ce8e-4512-9bdb-6c73d715feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data folder (update the variable to your path).\n",
    "path_data=os.path.join(\"..\", \"data\")\n",
    "# Seed value\n",
    "seed=22\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "\n",
    "image_width = 1024\n",
    "image_height = 1942\n",
    "scale_factor = 0.1\n",
    "\n",
    "valid_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "dataset_mean = [0.2391, 0.4028, 0.4096]\n",
    "dataset_std = [0.2312, 0.3223, 0.3203]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((dataset_mean),(dataset_std)),\n",
    "                                transforms.Resize((int(image_width*scale_factor), int(image_height*scale_factor)))])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=path_data, \n",
    "                                 transform=transform)\n",
    "\n",
    "n_val = int(np.floor(valid_size * len(dataset)))\n",
    "n_test = int(np.floor(test_size * len(dataset)))\n",
    "n_train = len(dataset) - n_val - n_test\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c19bcb-6b39-4258-9d5b-157dab7bc253",
   "metadata": {},
   "source": [
    "**Hint:** before proceeding, make sure that your `Edge`,`Cloud`, and `Good` samples are well enough among the `train`, `valid`,`test` splits. To print datasets statistics, run the next line.  Remember that the number of images in the different splits is distributed as described above. <br> If you are not happy with the data distribution, you can update the seed used and create a new dataset by rerunning the cell above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b0127d-d729-48e7-a1c4-557606850ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:  {0: 69, 1: 72, 2: 73}\n",
      "Test split:  {0: 14, 1: 16, 2: 15}\n",
      "Validation split:  {0: 13, 1: 14, 2: 18}\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "\n",
    "# Train loader\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Cross validation data loader\n",
    "valid_loader = DataLoader(val_ds, batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Test data loader\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "\n",
    "#0 - cloud\n",
    "#1 - edge\n",
    "#2 - good\n",
    "\n",
    "unique, counts = np.unique(torch.tensor([train_ds.dataset.targets[i] for i in train_ds.indices]), return_counts=True)\n",
    "print(\"Train split: \", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(torch.tensor([test_ds.dataset.targets[i] for i in test_ds.indices]), return_counts=True)\n",
    "print(\"Test split: \", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(torch.tensor([val_ds.dataset.targets[i] for i in val_ds.indices]), return_counts=True)\n",
    "print(\"Validation split: \", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c1e33-6c9d-4020-bb4b-9a95cd7e77fa",
   "metadata": {},
   "source": [
    "### 2.2. - Create data loaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d5c989-13e1-48b6-bb99-b09027a94675",
   "metadata": {},
   "source": [
    "The next lines will create a dataloader. A data loader is used to break the dataset into batches of a size `batch_size`. <br> This is useful to ensure that your dataset will fit into your memory and to create a \"stochastic\" implementation of gradient descent. <br> For more information, please, check: [data loader](https://www.educative.io/answers/what-is-pytorch-dataloader).<br>\n",
    "Specify `batch_size` (**Hint**: use powers of 2. Typical values are between 8 and 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e0f8a6d-d620-424d-abae-48789e9c64f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split:  {0: 69, 1: 72, 2: 73}\n",
      "Test split:  {0: 14, 1: 16, 2: 15}\n",
      "Validation split:  {0: 13, 1: 14, 2: 18}\n"
     ]
    }
   ],
   "source": [
    "# Train loader\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Cross validation data loader\n",
    "valid_loader = DataLoader(val_ds, batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "# Test data loader\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, pin_memory=False, shuffle=True)\n",
    "\n",
    "#0 - cloud\n",
    "#1 - edge\n",
    "#2 - good\n",
    "\n",
    "unique, counts = np.unique(torch.tensor([train_ds.dataset.targets[i] for i in train_ds.indices]), return_counts=True)\n",
    "print(\"Train split: \", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(torch.tensor([test_ds.dataset.targets[i] for i in test_ds.indices]), return_counts=True)\n",
    "print(\"Test split: \", dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(torch.tensor([val_ds.dataset.targets[i] for i in val_ds.indices]), return_counts=True)\n",
    "print(\"Validation split: \", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec454c-0b6d-411c-bc63-7223de91dabd",
   "metadata": {},
   "source": [
    "## 3 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b7c14-2bf3-47c7-a2ff-ab80c5aa2c0a",
   "metadata": {},
   "source": [
    "Now, it is your turn! Add your code below to load a Neural Network model, select optimizers, learning rate and perform training. <br>\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63443f-23d3-4bb8-aab0-d563092b928d",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c1aba-ab0e-4d27-b704-6b72c3cca542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\20181212/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU to train model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20181212\\AppData\\Local\\anaconda3\\envs\\OMICRON\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 6.82, Validation Loss: 6.87, accuracy = 0.00\n",
      "Epoch: 2, Training Loss: 6.67, Validation Loss: 6.83, accuracy = 0.00\n",
      "Epoch: 3, Training Loss: 6.52, Validation Loss: 6.74, accuracy = 0.00\n",
      "Epoch: 4, Training Loss: 6.39, Validation Loss: 6.64, accuracy = 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "    \n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18')\n",
    "    \n",
    "import torch.optim as optim\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0003)\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=100, device=\"cpu\"):\n",
    "    \n",
    "    plotting = True\n",
    "\n",
    "    if plotting == True:\n",
    "       \n",
    "        # make filename to store figure\n",
    "        timestr = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        filename = './result_plots/training_plot_' + timestr + '.png'\n",
    "\n",
    "        # intialize plot\n",
    "        plt.ion()\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.legend([' (red)Training Loss', '(green)Validation Loss', '(blue)Accuracy'])\n",
    "        ax.set_ylim([0, 1.5])\n",
    "\n",
    "        # x - axis :array with epochs\n",
    "        epoch_arr = np.arange(1, epochs+1)\n",
    "\n",
    "        # y-axis: storing intermediate results for plotting \n",
    "        cum_train_loss = []\n",
    "        cum_val_loss = []\n",
    "        acc = [] \n",
    "\n",
    "\n",
    "        \n",
    "    # loop over epochs\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # training loop\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "            \n",
    "        training_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # validation loop\n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        \n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        \n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))\n",
    "\n",
    "\n",
    "        if plotting == True:\n",
    "            \n",
    "            # storing intermediate results for plotting\n",
    "            cum_train_loss.append(training_loss)\n",
    "            cum_val_loss.append(valid_loss)\n",
    "            acc.append(num_correct / num_examples)\n",
    "\n",
    "            # plotting results \n",
    "            ax.plot(epoch_arr[:epoch], cum_train_loss, 'r',linewidth=1)\n",
    "            ax.plot(epoch_arr[:epoch], cum_val_loss, 'g',linewidth=1)\n",
    "            ax.plot(epoch_arr[:epoch], acc, 'b',linewidth=1)\n",
    "\n",
    "            # refresh figure\n",
    "            fig.canvas.draw()\n",
    "            fig.canvas.flush_events()\n",
    "\n",
    "            # saving intermediate plots\n",
    "            if epoch % 10 == 0:\n",
    "                fig.savefig(filename)\n",
    "\n",
    "\n",
    "    # saving final plot \n",
    "    fig.savefig()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using GPU to train model\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"using CPU to train model\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train(model, optimizer, torch.nn.CrossEntropyLoss(),train_loader, valid_loader, epochs=500, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edec249-b41d-42f1-8d7c-fa8b9eb63393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test(model, test_loader, loss_fn, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    num_correct = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = num_correct / num_examples\n",
    "    print('Test Loss: {:.2f}, Test Accuracy: {:.2f}'.format(test_loss, accuracy))\n",
    "\n",
    "test(model, test_loader, torch.nn.CrossEntropyLoss(), device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adef2cd-09a3-4a1f-8554-46f97ab63841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
